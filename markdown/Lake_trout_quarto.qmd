---
title: "Lester Model plots"
format: 
  html:
    self-contained: true
editor: visual
---

## Lake Trout Lester Model

I want to commend the Region 3 staff for identifying the Lester Model as a candidate to improve lake trout management in Alaska. I've enjoyed reviewing the literature you sent and agree that the Lester model is a significant improvement over the lake area model currently in use. The question posed to me was whether it was appropriate to use the Lester model strait "out of the box" or if we need to update and/or ground truth the model before it is used for management. Bottom line up front: I do think it is worth the effort to use as much Alaskan data as possible to either modify or ground truth the estimates coming out of the Lester model before we use the results to take management action.

### Model intuition

One major advantage to using a model like this to manage our lake trout populations is that it can get us thinking about what population bottlenecks might exist and/or what research we might do to improve our assessment. I don't have the local area or biological experience to answer those types of questions but I did create a few plots to help demonstrate what the model thinks is going on for the 10 populations James included in this email. For starters, here is the lake specific information I was given (first 5 columns) as well as the model derived estimates that go into the MSY calculation (next 11 columns). The column names should match the definitions provided in the Lester paper.

```{r}
#| warning: FALSE
library(tidyverse)
library(knitr)
source("..\\functions\\lester_model_R_code.R")
```

```{r}
#| tab-cap: "Region 3 Lake trout habitat information and Lester model derived parameters."
lake <- c("Crosswind", "Fielding", "Glacier Gap", "Louise", "Paxson", "Little Sevenmile", "Summit", "Susitna", "Round Tangle", "Shallow Tangle", "Combined Tangle")
temp <- c(-3.04, -5.89, -7.13, -3.29, -4.15, -5.89, -5.89, -3.29, -7.13, -7.13, -7.13)
area <- c(3716.55, 561.96, 178.06, 5913.07, 1569.92, 35.13, 1770.12, 3635.16, 156.15, 129.5, 285.65)
mean_depth <- c(15.9, 8.7, 7.1, 13, 8.4, 4.4, 15.6, 9, 10, 2, 6.4)
max_depth <- c(36.6, 23.1, 24.4, 51, 29.7, 14.1, 63.4, 36.6, 27.3, 19.8, 27.3)
lester <- lester_msy(lake, temp, area, mean_depth, max_depth)
kable(lester, digits = 2)
```

I'll use a series of graphs to show 2 things that might be of interest while we start to think about ways to update and/or ground truth the Lester model. First, I'll estimate Lester model parameters over a grid of likely values to show the response surface for many derived parameters as contour plots. My hope is that staff can use these to develop intuition about how the data is used by the mode to estimate parameters of interest. Certain points within each contour plot will be identified so that we can see why the model believes some Region 3 lakes are more or less productive.

Thermocline depth is a parameter the model uses to partition habitat in each lake. As I understand it some of the habitat above the thermocline is used as a proxy for lake productivity while some of the habitat below is uses a a proxy for summer refuge habitat. The graph below shows the model estimated thermocline depths across a range of input values (as contours) and where each Region 3 lake fits on that relationship.

```{r}
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "Estimated thermocline depth for Region 3 lake trout habitats"
lester_plot <- 
  lester %>%
    mutate(#D_mn = DescTools::RoundTo(D_mn, 1.5),
           #W_inf = DescTools::RoundTo(W_inf, 0.25),
           pV_eb = round(pV_eb/0.2) * 0.2,
           S = round(S/0.25) * 0.25,
           lake_size = cut(A, c(0, 100, 1000, 6000),
                           c("0-100ha", "101-1000ha", "1001-6000ha")))

expand.grid(Temp = seq(-10, 5, 1), D_mn = seq(1, 17, 2), A = seq(100, 6100, 500)) %>% 
  mutate(D_th = 3.26*A^0.109*D_mn^0.213*exp(-0.0263*Temp)) %>% 
  ggplot(aes(x = Temp, y = D_mn, z = D_th)) + 
  geom_contour_filled() +
  geom_point(data = lester_plot, mapping = aes(color = lake), size = 3) +
  labs(title = "Thermocline Depth",
       x = "Mean Annual Air Temperature",
       y = "Mean lake depth") +
  facet_grid(. ~ lake_size, , labeller = label_both)
```

Here is the model estimated distribution of habitat types for the region 3 Lakes. The model uses epilimnion volume as a measure of lake productivity and hypolimnion volume as a measure of summer refuge habitat. The former is treated as proportional to MSY while the later becomes important only when mean annual air temperature is high.

```{r}
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "Lester model estimated epilimnion, metalimnion, and hypolimnion volume for region 3 lake trout populations"
lester %>% 
  mutate(pV_me = 1 - pV_eb - pV_hy) %>%
  select(lake, starts_with("pV")) %>%
  pivot_longer(cols = starts_with("pV")) %>%
  mutate(zone = factor(name, levels = c("pV_eb", "pV_me", "pV_hy"), labels = c("Epilimnion", "Metalimnion", "Hypolimnion"), ordered = TRUE)) %>%
  ggplot(aes(x = lake, y = value, fill = zone)) + 
    geom_col() +
    labs(title = "Habitat distibution",
         x = "Lake",
         y = "Percent of Volume") +
    theme(axis.text.x = element_text(angle = 25, vjust = 0.5, hjust=1))
```

As discussed, the habitat suitability index requires increased hypolimnion volume when air temperatures are high. Susitna, Little Sevenmile and Shallow Tangle are the lakes with questionable habitat according to the Lester model.

```{r}
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "Estimated lake trout habitat suitability as a function of mean annual air temperature and the percent of the lake volume which is in the hypolimnion zone (a measure of summertime habitat)"
expand.grid(Temp = seq(-10, 5, 1), pV_hy = seq(0, .4, .1)) %>% 
  mutate(S = 1/(1+ exp(2.47 + .389*Temp - 16.8 * pV_hy))) %>% 
  ggplot(aes(x = Temp, y = pV_hy, z = S)) + 
  geom_contour_filled() +
  geom_point(data = lester_plot, mapping = aes(color = lake), size = 3) +
  labs(title = "Lake Trout Habitat Suitability",
       x = "Mean Annual Air Temperature",
       y = "Proportion of the lake volume in the hypolimnetic zone")
```

Four variables go into the biomass density calculation so there is a lot going on in this plot. The relationship which may be most questionable for Alaska is whether epibenthic volume has the same relationship to productivity in Alaska that is has in Ontario.

```{r}
#| fig-cap: "Estimated lake trout biomass density as a function of mean depth, asymptotic weight, habitat suitability (S), and the percent of the lake volume which is in the epibenthic zone (pV_eb, a measure of productivity)"
#| fig-height: 8
#| fig-width: 8
expand.grid(D_mn = seq(1.5, 19, 2), pV_eb = seq(0.2, 1, .2), S = seq(0.5, 1, 0.25), W_inf = seq(0.75, 4.5, .25)) %>%
  mutate(B_msy = 8.47*(D_mn*pV_eb*S)/W_inf^1.33) %>% 
  ggplot(aes(x = D_mn, y = W_inf, z = B_msy)) + 
  geom_contour_filled(breaks = c(0, 2.5, 5, 10, 20, 200)) +
  geom_point(data = lester_plot, mapping = aes(color = lake), size = 3) +
  scale_x_continuous(limits = c(1.5, 19)) +
  facet_grid(S ~ pV_eb, , labeller = label_both) +
  labs(title = "Biomass density at MSY",
       x = "Mean depth",
       y = "Asymptotic weight")
```

I would guess we have a lot of local data we could use to inform an Alaska specific relationship between lake area, asymptotic length and asymptotic weight.

```{r}
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "Estimated asymptotic length and weight for region 3 lake trout populations"
lester %>% 
  select(lake, A, L_inf, W_inf) %>%
  pivot_longer(cols = ends_with("inf")) %>%
  ggplot(aes(x = A, y = value, color = lake)) + 
    geom_point() +
    facet_wrap(.~name, scales = "free") +
    labs(title = "Asymtotic length and weight",
         x = "Lake Area")
```

Note that while mortality is a function fo asymptotic weight and temperature it reduced to mostly a function of weights since our lakes are all similar temperatures.

```{r}
#| fig-cap: "Estimated lake trout natural mortality as a function of mean annual air temperature and asymptotic weight"
#| fig-height: 6
#| fig-width: 8
expand.grid(Temp = seq(-10, 5, 1), W_inf = seq(0.75, 4.5, .25)) %>% 
  mutate(M = 0.26*(exp(0.021*Temp+0.0004*Temp^2))/W_inf^0.30) %>% 
  ggplot(aes(x = Temp, y = W_inf, z = M)) + 
  geom_contour_filled() +
  geom_point(data = lester_plot, mapping = aes(color = lake), size = 3) +
  labs(title = "Lake Trout Natural Mortality",
       x = "Mean Annual Air Temperature",
       y = "Asymptotic weight")
```

```{r}
#| fig-cap: "Estimated MSP per hectare as a function of mortality and biomass density"
#| fig-height: 6
#| fig-width: 8
expand.grid(M = seq(.15, .25, .1), B_msy = seq(4, 20, 4)) %>% 
  mutate(msy_ha = B_msy*M) %>% 
  ggplot(aes(x = M, y = B_msy, z = msy_ha)) + 
  geom_contour_filled() +
  geom_point(data = lester_plot, mapping = aes(color = lake), size = 3) +
  labs(title = "MSY per hectare",
       x = "Mortality",
       y = "Biomass density")
```

### How to update

There are two ways to use Lester's model: (1) we have enough of our own data to fit the model and update parameters, or (2) we use the process Lester, Shuter, Jones, and Sandstrom took to create their model (e.g., look for relationships, create equations, estimate MSY, etc.) to create our own model for our Alaskan lake trout lakes. Note that both ways involve creating our own model, inspired by Lester et al.'s work.

1.  We have enough data from our Alaskan Lake trout lakes to fit the model and update parameters.
    1.  First, we look at the relationships between length infinity, weight infinity, thermocline depth, hypolimnetic volume, epibenthic volume, etc. to see if the relationships are the same for Alaskan lakes/lake trout.
        1.  If the relationships are the same, we use the Alaskan data to update parameter values.
        2.  If the relationships aren't the same, we update the equations suggested by Lester to reflect the relationships we see in our data. We may have to add in effects on lake trout survival from winter ice thickness, snow melt, etc. We then update parameter values in our new equations using Alaskan data.
    2.  We calculate sustained yield for each of our lakes, following Lester's suggestions.
    3.  We use our version of the Lester model to calculate MSY, and then compare it to sustained yield for our lakes.
        1.  Do our calculations for biomass and MSY make sense? Are there factors influencing lake trout survival that aren't in the model? How can we be sure that the resulting MSY is actually maximum sustained yield for our fish?
        2.  Regardless, use the MSY for individual lakes with extreme caution. If we want to know how a lake trout population is doing, then we need abundance and harvest data on that population.
        3.  Since data on multiple lakes were used in the model, our population of inference is all the Alaskan lakes in our dataset. Thus, the MSY calculated from our own model using Alaskan data is a description of all our Alaskan lakes and not just one. Lake information plugged into the model gives average MSY for Alaskan lakes with the same information as was plugged in. Notice that the MSY applies to lakes of a certain size, depth, etc. in general and may not be true for a specific lake of the same size, depth, etc.
2.  We don't have much data on Alaskan lakes, and may never have enough. We use the process Lester et al. took to create their model.
    1.  We determine average annual sustainable exploitation rates for the lakes where the abundance hasn't changed and we have sufficient abundance estimates and SWHS estimates.
    2.  We explore relationships between these exploitation rates and the other data we have on length, weight, lake depth, etc.
    3.  We ask ourselves: Are these relationships strong enough to tell us anything about our lakes and lake trout populations?
        1.  If so, create equations based on the relationships we see to create our own model. Update parameter values in the equations based on our Alaskan data. Then use our model to calculate sustainable exploitation rates for our lakes.
            1.  Do those sustainable exploitation rates make sense based on what we do know about those lakes?
            2.  Regardless, use the sustainable exploitation rates for an individual lake calculated in this way with extreme caution. If we want to know how a lake trout population is doing, then we need abundance and harvest data on that population.
            3.  Since data on multiple lakes were used in the model, our population of inference is all the Alaskan lakes in our dataset. Thus, the sustainable exploitation rates calculated from our own model using Alaskan data is a description of all our Alaskan lakes and not just one. Lake information plugged into the model gives average sustainable exploitation rates for Alaskan lakes with the same information as was plugged in. Notice that the sustainable exploitation rate applies to lakes of a certain size, depth, etc. in general and may not be true for a specific lake of the same size, depth, etc.
        2.  If not, we need to collect more data within our budget to update the model, or be content calculating sustainable exploitation rates for an individual lake using abundance and SWHS estimates for said lake and then using those exploitation rates as a management tool for that specific lake.

### How to ground truth

The other issue I think we need to consider is how to verify the model is making accurate management recommendations for our lakes. Option 1 under the "How to update" section is more tricky and requires a lot of data and validation. We have to make sure we are seeing strong relationships in our data, which then create the equations that make up the model. Option 2 allows us to use the data we have but can be equally tricky. We still have to make sure we are seeing strong relationships in our data, which then create the equations that make up the model.

### How to not abuse the Lester model

I think it's important to note that Lester et al. intended this model be be a regional scale diagnostic, which is indicated by the title of Lester et al.'s paper (i.e., "A General, Life History Based Model for Sustainable Exploitation of Lake Charr across their Range"). Notice that the model is being developed with the goal of finding sustainable exploitation for Lake Charr across their range, although the authors recognize how landscape variation can affect Lake Charr population dynamics. They also felt further validation of the biomass sub-model for colder northern regions was critical, because much of the data to inform this model came from lakes in the southern portion of the species' range. They felt the MSY estimates were too variable to be of much use as well. The model is mostly heuristic in that no estimates of variability are produced and we really can't say much about the quality of its estimates. I hear staff talking about wanting to use these estimates of MSY to modify regulations on individual lakes. I'd urge caution here and think hard about how to make decisions for groups of similar lakes. The Lester paper gives some instruction on how they thought this should be done.

When I look at Jordy's comparison between the lake area and Lester models, I see Crosswind and Louise as the largest difference on the kilogram scale but see Shallow Tangle and Little Sevenmile as the largest proportional changes from the LAM estimates. Do we have the management resolution to respond to the changes we are seeing for the smaller MSY lakes? Is there a pattern in the model differences wrt population viability, accessibility, or fishing pressure that we can manage to?

The other thing I noticed is that Jordy was considering different scaling factors for harvest targets as a percent of MSY. If we are assuming the yield potential from the LAM is half of MSY and also trying to make sure our harvest stays below LAM yield potential, then scaling factors above 0.5 represent liberalizations that we are making by choice, rather than as a result of this new assessment.
